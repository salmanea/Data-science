{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation using Ollama in Postgres\n",
    "\n",
    "## Motivation\n",
    "Have you ever struggled to find information across different systems in your organization? Many organizations face this critical challenge: their valuable knowledge is scattered across different systems and formats. To find the information they need, they often have to search through multiple systems, which is time-consuming and inefficient.\n",
    "\n",
    "pgvector brings vector search capabilities to PostgreSQL, enabling fast and accurate information retrieval using large language models (LLMs). When used in a Retrieval-Augmented Generation (RAG) setup, pgvector helps you build AI-powered search solutions that index and retrieve organizational knowledge with precision.\n",
    "\n",
    "In this article, we'll build a simple RAG system using pgvector and Ollama.\n",
    "\n",
    "## Tools Used in This Article\n",
    "\n",
    "### What Is Ollama?\n",
    "\n",
    "Ollama is a lightweight, open-source model server that allows you to run and interact with large language models (LLMs) on your local machine. It provides a simple interface for running and managing LLMs, making it easy to experiment with different models and fine-tune them for your specific use cases.\n",
    "\n",
    "For more information on using Ollama in private AI workflows, check out this [guide](https://codecut.ai/private-ai-workflows-langchain-ollama).\n",
    "\n",
    "### What Is pgvector?\n",
    "\n",
    "[pgvector](https://github.com/pgvector/pgvector) is a PostgreSQL extension that provides vector similarity search capabilities. It allows you to store and query vector embeddings in your PostgreSQL database.\n",
    "\n",
    "[pgvector-python](https://github.com/pgvector/pgvector-python) is a Python library for pgvector. It allows you to use pgvector with SQLAlchemy, making it easy to integrate pgvector into your existing Python applications.\n",
    "\n",
    "### What Is SQLAlchemy?\n",
    "\n",
    "[SQLAlchemy](https://www.sqlalchemy.org/) is a Python library for interacting with databases. It provides a simple interface for executing SQL queries and managing database connections.\n",
    "\n",
    "\n",
    "## Project Architecture\n",
    "\n",
    "### Goals\n",
    "\n",
    "Our goal is to get an LLM to answer questions about a document. We'll use the [A Deep Dive into DuckDB for Data Scientists](https://codecut.ai/deep-dive-into-duckdb-data-scientists/) article as our document.\n",
    "\n",
    "### Data\n",
    "\n",
    "We'll use the [A Deep Dive into DuckDB for Data Scientists](https://codecut.ai/deep-dive-into-duckdb-data-scientists/) article as our document.\n",
    "\n",
    "### RAG Architecture\n",
    "\n",
    "1. **Data Preparation:**\n",
    "\n",
    "    - Download the document and convert it to markdown.\n",
    "    - Split the document into chunks, each containing a single section of the document.\n",
    "\n",
    "2. **Embedding:**\n",
    "\n",
    "    - Embed the chunks using the Ollama 'nomic-embed-text' model.\n",
    "\n",
    "3. **Vector Storage:**\n",
    "\n",
    "    - Create PostgreSQL table with vector column using pgvector.\n",
    "    - Store document chunks and their embeddings in the table.\n",
    "\n",
    "4. **Query Process:**\n",
    "\n",
    "    - Convert user query to embedding using Ollama.\n",
    "    - Use vector similarity search to find the most relevant chunks to the query.\n",
    "\n",
    "5. **Response Generation:**\n",
    "\n",
    "    - Retrieve the most relevant chunks from the database.\n",
    "    - Use Ollama's Llama 3.2 model to generate a response to the query.\n",
    "\n",
    "### RAG Workflow\n",
    "\n",
    "Here is a diagram of the RAG workflow:\n",
    "\n",
    "```{mermaid}\n",
    "graph TD\n",
    "    %% Data Preparation\n",
    "    A[Document] -->|Split by Sections| B[Document Chunks]\n",
    "\n",
    "    %% Embedding\n",
    "    B -->|nomic-embed-text| C[Document Embeddings]\n",
    "    \n",
    "    %% Vector Storage\n",
    "    C -->|Store| F[(PostgreSQL)]\n",
    "    \n",
    "    %% Query Process\n",
    "    H[User Query] -->|nomic-embed-text| J[Query Embedding]\n",
    "    J -->|Vector Similarity Search| F\n",
    "    F -->|Retrieve| K[Relevant Chunks]\n",
    "    \n",
    "    %% Response Generation\n",
    "    K -->|Input| M[LLM Model]\n",
    "    M -->|Generate| L[Response]\n",
    "\n",
    "    classDef pink stroke-width:2px, stroke-dasharray: 0, fill:#E583B6, stroke:#FFFFFF, color:#FFFFFF\n",
    "    classDef blue stroke-width:2px, stroke-dasharray: 0, stroke:#FFFFFF, fill:#72BEFA, color:#FFFFFF\n",
    "    classDef green stroke-width:2px, stroke-dasharray: 0, stroke:#FFFFFF, fill:#72FCDB, color:#FFFFFF\n",
    "    \n",
    "    %% Styling\n",
    "    class A,B,C pink\n",
    "    class F,K,L,M blue\n",
    "    class H,J green\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Setup Ollama\n",
    "\n",
    "Go to the [Ollama website](https://ollama.ai/download) and download the appropriate version for your operating system.\n",
    "\n",
    "Download the [nomic-embed-text](https://ollama.ai/library/nomic-embed-text) model for embedding text:\n",
    "\n",
    "```bash\n",
    "ollama pull nomic-embed-text\n",
    "```\n",
    "\n",
    "Download the [llama3.2](https://ollama.ai/library/llama3.2) model for generating responses:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "Start the Ollama server:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "### Setup pgvector\n",
    "\n",
    "Install pgvector:\n",
    "\n",
    "```bash\n",
    "brew install pgvector\n",
    "```\n",
    "\n",
    "### Install Python Packages\n",
    "\n",
    "Install required Pythonpackages:\n",
    "\n",
    "```bash\n",
    "pip install pgvector-python sqlalchemy psycopg2-binary\n",
    "```\n",
    "\n",
    "### Setup PostgreSQL Database\n",
    "\n",
    "For Mac, install PostgreSQL using Homebrew:\n",
    "\n",
    "```bash\n",
    "brew install postgresql\n",
    "```\n",
    "\n",
    "For other operating systems, see the [PostgreSQL documentation](https://www.postgresql.org/download/).\n",
    "\n",
    "Create a database used for this project:\n",
    "\n",
    "```bash\n",
    "createdb pgvector_example\n",
    "```\n",
    "\n",
    "## RAG Implementation\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "Import libraries used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "from markdownify import markdownify as md\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy import Column, Integer, Text, create_engine, select, text\n",
    "from sqlalchemy.orm import Session, declarative_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Data\n",
    "\n",
    "Start by downloading the article content from the specified URL and convert it to Markdown format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download HTML\n",
    "url = \"https://codecut.ai/deep-dive-into-duckdb-data-scientists/\"\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "article_html = resp.text\n",
    "\n",
    "# Convert to Markdown\n",
    "article_md = md(article_html, heading_style=\"ATX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the content by removing everything before the introduction and after key takeaways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove everything before the first header and after key takeaways\n",
    "intro_start = \"## Introduction\"\n",
    "key_takeaways = \"## Key Takeaways\"\n",
    "start_idx = article_md.find(intro_start)\n",
    "end_idx = article_md.find(key_takeaways)\n",
    "cleaned_md = article_md[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned content to a local Markdown file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .md\n",
    "with open(\"deep_dive_duckdb.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_md)\n",
    "\n",
    "print(\"Saved article as deep_dive_duckdb.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Database\n",
    "\n",
    "Use SQLAlchemy to connect to the `pgvector_example` database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine\n",
    "engine = create_engine(\"postgresql://localhost/pgvector_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pgvector extension, which adds vector operations and similarity search capabilities to PostgreSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extension\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into Chunks\n",
    "\n",
    "We'll split the data into chunks to create smaller, manageable pieces of text that can be embedded and stored in the vector database.\n",
    "\n",
    "We specifically split by the header '##' because headers naturally divide content into logical sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into chunks by the header ##\n",
    "chunks = cleaned_md.split(\"\\n## \")\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"First chunk: {chunks[0][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "Number of chunks: 18\n",
    "First chunk: ## Introduction\n",
    "\n",
    "Are you tired of the overhead of setting up database servers just to run SQL querie\n",
    "```\n",
    "### Embed the Chunks\n",
    "\n",
    "Text embedding is the process of converting text into numerical vectors that computers can process. Think of it like translating human language into a mathematical format that captures the meaning and context of the text.\n",
    "\n",
    "Just as GPS coordinates pinpoint a location on Earth, embeddings create a unique numerical \"address\" for each piece of text in a high-dimensional space. These vectors preserve semantic relationships - similar texts will have similar vector representations.\n",
    "\n",
    "For this project, we'll use Ollama's `nomic-embed-text` model to generate these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a prefix to the chunks\n",
    "documents = [\"search_document: \" + chunk for chunk in chunks]\n",
    "\n",
    "# Embed the chunks using the ollama model\n",
    "document_embeddings = ollama.embed(model=\"nomic-embed-text\", input=documents).embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our database model and connection using SQLAlchemy by defining a `Chunk` model that represents how our data will be stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base class for declarative models\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Chunk(Base):\n",
    "    __tablename__ = \"chunks\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    content = Column(Text)\n",
    "    embedding = Column(Vector(768))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the table in the database and set up our database connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table if it exists\n",
    "Base.metadata.drop_all(engine)\n",
    "\n",
    "# Create the table in the database\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Create a session to interact with the database\n",
    "session = Session(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the Chunks in the Database\n",
    "\n",
    "Store the text chunks and their corresponding embeddings in the PostgreSQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks and add them to the session\n",
    "for content, embedding in zip(chunks, document_embeddings, strict=False):\n",
    "    chunk = Chunk(content=content, embedding=embedding)\n",
    "    session.add(chunk)\n",
    "\n",
    "# Commit the changes\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first 10 rows of the `chunks` table as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM chunks LIMIT 10\", engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "| id  | content                                                                                      | embedding               |\n",
    "|-----|----------------------------------------------------------------------------------------------|-------------------------|\n",
    "| 1   | ## Introduction\\n\\nAre you tired of the overhead...                                          | [-0.016466418,...]      |\n",
    "| 2   | What is DuckDB?\\n\\n[DuckDB](https://github.com...)                                           | [0.0017575845,...]      |\n",
    "| 3   | Zero Configuration\\n\\nSQL operations on DataFrames...                                        | [0.006731806,...]       |\n",
    "| 4   | Integrate Seamlessly with pandas and Polars\\n\\n...                                           | [0.045110848,...]       |\n",
    "| 5   | Memory Efficiency\\n\\nA major drawback of pandas...                                           | [0.0076164557,...]      |\n",
    "| 6   | Fast Performance\\n\\nWhile pandas processes data...                                           | [0.012767632,...]       |\n",
    "| 7   | Streamlined File Reading\\n\\n### Automatic Parsing...                                         | [0.0068280566,...]      |\n",
    "| 8   | Writing the CSV content to a file\\nwith open(\"...\")                                          | [0.014320952,...]       |\n",
    "| 9   | Reading the CSV file with pandas without specifying...                                       | [0.008751955,...]       |\n",
    "| 10  | Use DuckDB to automatically detect and read the file...                                      | [0.04343769,...]        |\n",
    "\n",
    "### Query the Database\n",
    "\n",
    "Similar to how we embedded the chunks, we'll embed the query so that computers can understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is DuckDB?\"\n",
    "input_query = \"search_query: \" + query\n",
    "query_embedding = ollama.embed(model=\"nomic-embed-text\", input=input_query).embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to retrieve the most similar chunks. We'll use the `l2_distance` function (also known as Euclidean distance) to find the closest chunks to the query. \n",
    "\n",
    "This function measures the straight-line distance between two vectors in n-dimensional space, where n is the dimension of our embeddings (768 in this case). The smaller the distance, the more similar the vectors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the most similar chunks\n",
    "results = session.scalars(\n",
    "    select(Chunk).order_by(Chunk.embedding.l2_distance(query_embedding))\n",
    "    .order_by(Chunk.embedding.l2_distance(query_embedding))\n",
    "    .limit(5)\n",
    ").all()\n",
    "\n",
    "# Join the results into a single string\n",
    "context = \"\\n\\n\".join([f\"{i+1}. {row.content}\" for i, row in enumerate(results)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first 100 characters of the context for each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(results):\n",
    "    print(f\"{i+1}. {result.content[:100]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "1. What is DuckDB?\n",
    "\n",
    "[DuckDB](https://github.com/duckdb/duckdb) is a fast, in-process SQL OLAP database \n",
    "\n",
    "2. ## Introduction\n",
    "\n",
    "Are you tired of the overhead of setting up database servers just to run SQL querie\n",
    "\n",
    "3. Use DuckDB to automatically detect and read the CSV structure\n",
    "result = duckdb.query(\"SELECT * FROM r\n",
    "\n",
    "4. Fast Performance\n",
    "\n",
    "While pandas processes data sequentially row-by-row, DuckDB uses a vectorized exec\n",
    "\n",
    "5. Reading Multiple Files\n",
    "\n",
    "### Reading Multiple Files from a Directory\n",
    "\n",
    "It can be complicated to read m\n",
    "```\n",
    "\n",
    "We can see that the context is a list of chunks that are similar to the query ordered by similarity.\n",
    "\n",
    "pgvector also supports other distance functions such as `max_inner_product`, `cosine_distance`, `l1_distance`, `hamming_distance`, and `jaccard_distance`.\n",
    "\n",
    "### Generate a Response\n",
    "\n",
    "Now, we are ready to use the context to generate a response to the query. We'll use the `llama3.2` model to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Answer this question using only the following context: {context}\\n\\n{query}\"\n",
    "response = ollama.generate(model=\"llama3.2\", prompt=prompt).response\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "Query: What is DuckDB?\n",
    "Response: DuckDB is a fast, in-process SQL OLAP database optimized for analytics. It offers several key features that make it an attractive option for data scientists, including:\n",
    "\n",
    "1. Zero Configuration: No need to set up or maintain a separate database server.\n",
    "2. Memory Efficiency: Process large datasets without loading everything into memory.\n",
    "3. Familiar Interface: Use SQL syntax you already know, directly in Python.\n",
    "4. Performance: Faster than pandas for many operations, especially joins and aggregations.\n",
    "5. File Format Support: Direct querying of CSV, Parquet, and other file formats.\n",
    "\n",
    "DuckDB provides a streamlined approach to data querying without the overhead of setting up database servers. It significantly outperforms pandas in terms of both performance and memory efficiency, especially when handling complex operations like joins and aggregations on large datasets.\n",
    "```\n",
    "\n",
    "The response is accurate and relevant to the query!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "This is a simple example of how to use pgvector to create a RAG system. There are many ways to improve this example. Here are some ideas:\n",
    "\n",
    "- Replace the article with your own content and ask custom questions.\n",
    "- Try different embedding models or distance functions (e.g., cosine_distance).\n",
    "- Experiment with chunk sizes and splitting strategies.\n",
    "\n",
    "That's it for this example! I hope you found it helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/.pyenv/versions/3.11.2/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}